{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 : To predict the genre for test data across all the saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/home/cse587/spark-2.4.0-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pyspark' from '/home/cse587/spark-2.4.0-bin-hadoop2.7/python/pyspark/__init__.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "\n",
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "   .appName(\"Assignment3p4testing\") \\\n",
    "   .config(\"spark.some.config.option\", \"8gb\") \\\n",
    "   .getOrCreate()\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlcontext=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading necessary libraries for this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "import re\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.sql.functions import udf, col, lower, regexp_replace\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, VectorIndexer\n",
    "from pyspark.mllib.linalg import SparseVector, DenseVector\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, VectorIndexer, IndexToString\n",
    "from pyspark.ml.feature import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get predictions using Model from part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_part1(training_df,testing_df,model):\n",
    "    ## Defining Extra stopwords to maintain consistency with the trained model\n",
    "    extra_stopwords =['http','https','amp','rt','t','c','the','despite','working','develop','whence', 'here', 'show', 'were', 'why', \n",
    "            'n’t', 'the', 'whereupon', 'not', 'more', 'how', 'eight', 'indeed', 'i', 'only', 'via', 'nine', \n",
    "            're', 'themselves', 'almost', 'to', 'already', 'front', 'least', 'becomes', 'thereby', 'doing', \n",
    "            'her', 'together', 'be', 'often', 'then', 'quite', 'less', 'many', 'they', 'ourselves', 'take',\n",
    "            'its', 'yours', 'each', 'would', 'may', 'namely', 'do', 'whose', 'whether', 'side', 'both', 'what',\n",
    "            'between', 'toward', 'our', 'whereby', \"'m\", 'formerly', 'myself', 'had', 'really', 'call', 'keep',\n",
    "            \"'re\", 'hereupon', 'can', 'their', 'eleven', '’m', 'even', 'around', 'twenty', 'mostly', 'did', \n",
    "            'at', 'an', 'seems', 'serious', 'against', \"n't\", 'except', 'has', 'five', 'he', 'last', '‘ve', \n",
    "            'because', 'we', 'himself', 'yet', 'something', 'somehow', '‘m', 'towards', 'his', 'six',\n",
    "            'anywhere', 'us', '‘d', 'thru', 'thus', 'which', 'everything', 'become', 'herein', 'one', 'in',\n",
    "            'although', 'sometime', 'give', 'cannot', 'besides', 'across', 'noone', 'ever', 'that', 'over', \n",
    "            'among', 'during', 'however', 'when', 'sometimes', 'still', 'seemed', 'get', \"'ve\", 'him', 'with', \n",
    "            'part', 'beyond', 'everyone', 'same', 'this', 'latterly', 'no', 'regarding', 'elsewhere', 'others', \n",
    "            'moreover', 'else', 'back', 'alone', 'somewhere', 'are', 'will', 'beforehand', 'ten', 'very',\n",
    "            'most', 'three', 'former', '’re', 'otherwise', 'several', 'also', 'whatever', 'am', 'becoming', \n",
    "            'beside', '’s', 'nothing', 'some', 'since', 'thence', 'anyway', 'out', 'up', 'well', 'it',\n",
    "            'various', 'four', 'top', '‘s', 'than', 'under', 'might', 'could', 'by', 'too', 'and',\n",
    "            'whom', '‘ll', 'say', 'therefore', \"'s\", 'other', 'throughout', 'became', 'your', 'put', 'per', \n",
    "            \"'ll\", 'fifteen', 'must', 'before', 'whenever', 'anyone', 'without', 'does', 'was', 'where', \n",
    "            'thereafter', \"'d\", 'another', 'yourselves', 'n‘t', 'see', 'go', 'wherever', 'just', 'seeming', \n",
    "            'hence', 'full', 'whereafter', 'bottom', 'whole', 'own', 'empty', 'due', 'behind', 'while', 'onto', \n",
    "            'wherein', 'off', 'again', 'a', 'two', 'above', 'therein', 'sixty', 'those', 'whereas', 'using', \n",
    "            'latter', 'used', 'my', 'herself', 'hers', 'or', 'neither', 'forty', 'thereupon', 'now', 'after',\n",
    "            'yourself', 'whither', 'rather', 'once', 'from', 'until', 'anything', 'few', 'into', 'such', 'being',\n",
    "            'make', 'mine', 'please', 'along', 'hundred', 'should', 'below', 'third', 'unless', 'upon', 'perhaps',\n",
    "            'ours', 'but', 'never', 'whoever', 'fifty', 'any', 'all', 'nobody', 'there', 'have', 'anyhow', 'of', \n",
    "            'seem', 'down', 'is', 'every', '’ll', 'much', 'none', 'further', 'me', 'who', 'nevertheless', 'about',\n",
    "            'everywhere', 'name', 'enough', '’d', 'next', 'meanwhile', 'though', 'through', 'on', 'first', 'been', \n",
    "            'hereby', 'if', 'move', 'so', 'either', 'amongst', 'for', 'twelve', 'nor', 'she', 'always', 'these', \n",
    "            'as', '’ve', 'amount', '‘re', 'someone', 'afterwards', 'you', 'nowhere', 'itself', 'done', 'hereafter',\n",
    "            'within', 'made', 'ca', 'them']\n",
    "    testing_df= testing_df.withColumn(\"plot\",F.lower(F.col('plot')))\n",
    "    regexTokenizer = RegexTokenizer(inputCol=\"plot\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "    remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\" ).setStopWords(extra_stopwords)\n",
    "\n",
    "    countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\",minDF=120)\n",
    "    pipeline = Pipeline(stages=[regexTokenizer, remover, countVectors])\n",
    "    \n",
    "    ## fitting the pipeline\n",
    "    data = pipeline.fit(training_df)\n",
    "    \n",
    "    rescaledData = data.transform(training_df)\n",
    "    label_stringIdx = StringIndexer(inputCol = \"genre\", outputCol = \"label\")\n",
    "    fitter = label_stringIdx.fit(rescaledData)\n",
    "#     rescaledData = fitter.transform(rescaledData)\n",
    "    labels = fitter.labels\n",
    "    \n",
    "    test_data = data.transform(testing_df)\n",
    "    ## fitting the model\n",
    "    test_pred = model.transform(test_data)\n",
    "#     print(\"Prediction from model of part1\")\n",
    "#     test_pred.select('movie_id','prediction').show()\n",
    "\n",
    "    ## converted the predicted labels to its corresponding genre\n",
    "    converter = IndexToString(inputCol=\"prediction\", outputCol=\"originalCategory\", labels = labels)\n",
    "    test_result = converter.transform(test_pred)\n",
    "    \n",
    "    return test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get predictions using Model from part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_part2(training_df,testing_df,model):\n",
    "    \n",
    "    tokenizer = Tokenizer(inputCol=\"plot\", outputCol=\"words\")\n",
    "    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[tokenizer, hashingTF, idf])\n",
    "    ## fitting the pipeline\n",
    "    \n",
    "    fitter = pipeline.fit(training_df)\n",
    "    \n",
    "    rescaledData = fitter.transform(training_df)\n",
    "    label_stringIdx = StringIndexer(inputCol = \"genre\", outputCol = \"label\")\n",
    "    lbl = label_stringIdx.fit(rescaledData)\n",
    "#     rescaledData = fitter.transform(rescaledData)\n",
    "    labels = lbl.labels\n",
    "    \n",
    "    \n",
    "    test_data2 = fitter.transform(testing_df)\n",
    "    ## fitting the model\n",
    "    test_pred = model.transform(test_data2)\n",
    "\n",
    "    ## converted the predicted labels to its corresponding genre\n",
    "    converter = IndexToString(inputCol=\"prediction\", outputCol=\"originalCategory\", labels = labels)\n",
    "    test_result = converter.transform(test_pred)\n",
    "\n",
    "    return test_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get predictions using Model from part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_part3(training_df,testing_df,model):\n",
    "    \n",
    "    regexTokenizer = RegexTokenizer(inputCol=\"plot\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "    word2vec = Word2Vec(vectorSize = 100, minCount = 5, inputCol = 'words', outputCol = 'features')\n",
    "\n",
    "    pipeline = Pipeline(stages=[regexTokenizer, word2vec])\n",
    "    ## fitting the pipeline\n",
    "    fitter = pipeline.fit(training_df)\n",
    "    \n",
    "    rescaledData = fitter.transform(training_df)\n",
    "    label_stringIdx = StringIndexer(inputCol = \"genre\", outputCol = \"label\")\n",
    "    lbl = label_stringIdx.fit(rescaledData)\n",
    "#     rescaledData = fitter.transform(rescaledData)\n",
    "    labels = lbl.labels\n",
    "    \n",
    "    test_data = fitter.transform(testing_df)\n",
    "    ## fitting the model\n",
    "    test_pred = model.transform(test_data)\n",
    "\n",
    "    ## converted the predicted labels to its corresponding genre\n",
    "    converter = IndexToString(inputCol=\"prediction\", outputCol=\"originalCategory\", labels = labels)\n",
    "    test_result = converter.transform(test_pred)\n",
    "\n",
    "    return test_result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining function to map and extract the final predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_and_extract(result):\n",
    "    mapping = pd.read_csv('/home/cse587/Project3/mapping.csv')\n",
    "    colnames = ['label','name']\n",
    "    mapping.columns =colnames\n",
    "    mapping_df = spark.createDataFrame(mapping)\n",
    "    def convert(lines):\n",
    "        lines = lines[1:-1].split(', ')\n",
    "        temp=[0]*20\n",
    "        for i in lines:   \n",
    "            for key,value in mapping.iterrows():\n",
    "                if i == ('\\''+ value['name'] +'\\''):\n",
    "                    temp[key] = 1\n",
    "                    continue\n",
    "        return \" \".join(map(str,temp))\n",
    "\n",
    "    converter = udf(convert)\n",
    "    result = result.withColumn(\"predictions\",converter('originalCategory'))\n",
    "    final_data = result.select(\"movie_id\",\"predictions\")\n",
    "    \n",
    "    return final_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading test data from local using Pandas (Enter the test data path here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = \"/home/cse587/Project3/test.csv\"\n",
    "test_data = pd.read_csv(test_data_path,sep=\",\", header = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating testing dataframe in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df = spark.createDataFrame(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and saving training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify training data path\n",
    "train_data = pd.read_csv(\"/home/cse587/Project3/train.csv\",sep=\",\", header = 0)\n",
    "training_df = spark.createDataFrame(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the result for each part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## paths of saved models\n",
    "path1 = \"/home/cse587/Project3/randomforest_for_part1\"\n",
    "path2 = \"/home/cse587/Project3/randomforest_for_part2\"\n",
    "path3 = \"/home/cse587/Project3/randomforest_for_part3\"\n",
    "\n",
    "## loading the saved models from all parts\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "saved_model1 = RandomForestClassificationModel.load(path1)\n",
    "saved_model2 = RandomForestClassificationModel.load(path2)\n",
    "saved_model3 = RandomForestClassificationModel.load(path3)\n",
    "\n",
    "## getting the predictions from all models for a given test data \n",
    "part1_result = prediction_part1(training_df,testing_df,saved_model1)\n",
    "part2_result = prediction_part2(training_df,testing_df,saved_model2)\n",
    "part3_result = prediction_part3(training_df,testing_df,saved_model3)\n",
    "\n",
    "## mapping the prediction to binary string using mapping.csv\n",
    "part1_final = map_and_extract(part1_result)\n",
    "part2_final = map_and_extract(part2_result)\n",
    "part3_final = map_and_extract(part3_result)\n",
    "\n",
    "## saving the results in .csv format , specify file name if required\n",
    "\n",
    "part1_final.write.csv('part1_test_result_rf_video.csv', header = True)\n",
    "part2_final.write.csv('part2_test_result_rf_video.csv', header = True)\n",
    "part3_final.write.csv('part3_test_result_rf_video.csv', header = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
